{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzw/miniconda3/envs/Bert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1,3'\n",
    "device = 'cuda'\n",
    "\n",
    "import peft\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoConfig\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained('model/chinese-llama-alpaca-plus-lora-7b/', device_map='auto')\n",
    "# base_model = LlamaForCausalLM.from_pretrained('decapoda-research/llama-7b-hf', device_map='auto')\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     base_model,\n",
    "#     'ziqingyang/chinese-llama-plus-lora-7b',\n",
    "#     device_map='auto',\n",
    "#     torch_dtype=torch.float16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.40s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./model/chinese-llama-alpaca-plus-lora-7b\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    torch_dtype = torch.float16\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        config=config,\n",
    "        torch_dtype=torch_dtype,\n",
    "        # low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = model.cuda()\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # 加载lora\n",
    "    peft_model_path = \"./model/ChatMed-Consult_llama_lora_pt_v0\"\n",
    "    model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 12&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 │   </span>device = torch.device(<span style=\"color: #808000; text-decoration-color: #808000\">'cuda'</span>)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 │   </span>inputs = tokenizer(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>(<span style=\"color: #808000; text-decoration-color: #808000\">''</span>), return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>, add_special_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>14 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>output = tokenizer(                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 │   │   </span>inputs.input_ids.to(device),                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 │   │   </span>inputs.attention_mask.to(device),                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 │   │   </span>eos_token_id=tokenizer.eos_token_id,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/lzw/miniconda3/envs/Bert/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_base.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2548</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2545 │   │   │   # input mode in this case.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2546 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._in_target_context_manager:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2547 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._switch_to_input_mode()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2548 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_one(text=text, text_pair=text_pair, **all_kwargs)      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2549 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text_target <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2550 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._switch_to_target_mode()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2551 │   │   │   </span>target_encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_one(text=text_target, text_pair=text_pair_targ  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/lzw/miniconda3/envs/Bert/lib/python3.8/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_base.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2606</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_one</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2603 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2604 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2605 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> _is_valid_text_input(text):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2606 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2607 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"text input must of type `str` (single example), `List[str]` (batch or s</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2608 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"or `List[List[str]]` (batch of pretokenized examples).\"</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2609 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>text input must of type `str` <span style=\"font-weight: bold\">(</span>single example<span style=\"font-weight: bold\">)</span>, `List<span style=\"font-weight: bold\">[</span>str<span style=\"font-weight: bold\">]</span>` <span style=\"font-weight: bold\">(</span>batch or single pretokenized example<span style=\"font-weight: bold\">)</span> or \n",
       "`List<span style=\"font-weight: bold\">[</span>List<span style=\"font-weight: bold\">[</span>str<span style=\"font-weight: bold\">]]</span>` <span style=\"font-weight: bold\">(</span>batch of pretokenized examples<span style=\"font-weight: bold\">)</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 12>\u001b[0m:\u001b[94m14\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   \u001b[0mdevice = torch.device(\u001b[33m'\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m'\u001b[0m)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   \u001b[0minputs = tokenizer(\u001b[96minput\u001b[0m(\u001b[33m'\u001b[0m\u001b[33m'\u001b[0m), return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m, add_special_tokens=\u001b[94mFalse\u001b[0m)            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m14 \u001b[2m│   \u001b[0moutput = tokenizer(                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   \u001b[0minputs.input_ids.to(device),                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0minputs.attention_mask.to(device),                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0meos_token_id=tokenizer.eos_token_id,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/lzw/miniconda3/envs/Bert/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m2548\u001b[0m in \u001b[92m__call__\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2545 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# input mode in this case.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2546 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._in_target_context_manager:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2547 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._switch_to_input_mode()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2548 \u001b[2m│   │   │   \u001b[0mencodings = \u001b[96mself\u001b[0m._call_one(text=text, text_pair=text_pair, **all_kwargs)      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2549 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m text_target \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2550 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._switch_to_target_mode()                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2551 \u001b[0m\u001b[2m│   │   │   \u001b[0mtarget_encodings = \u001b[96mself\u001b[0m._call_one(text=text_target, text_pair=text_pair_targ  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/lzw/miniconda3/envs/Bert/lib/python3.8/site-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m2606\u001b[0m in \u001b[92m_call_one\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2603 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mFalse\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2604 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2605 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m _is_valid_text_input(text):                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2606 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2607 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mtext input must of type `str` (single example), `List[str]` (batch or s\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2608 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mor `List[List[str]]` (batch of pretokenized examples).\u001b[0m\u001b[33m\"\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2609 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mtext input must of type `str` \u001b[1m(\u001b[0msingle example\u001b[1m)\u001b[0m, `List\u001b[1m[\u001b[0mstr\u001b[1m]\u001b[0m` \u001b[1m(\u001b[0mbatch or single pretokenized example\u001b[1m)\u001b[0m or \n",
       "`List\u001b[1m[\u001b[0mList\u001b[1m[\u001b[0mstr\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m` \u001b[1m(\u001b[0mbatch of pretokenized examples\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generation_config = dict(\n",
    "    temperature=0.2,\n",
    "    # top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.3,\n",
    "    max_new_tokens=400\n",
    ")\n",
    "\n",
    "while True:\n",
    "    device = torch.device('cuda')\n",
    "    inputs = tokenizer(input(''), return_tensors=\"pt\", add_special_tokens=False)\n",
    "    output = tokenizer(\n",
    "        inputs.input_ids.to(device),\n",
    "        inputs.attention_mask.to(device),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **generation_config\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
