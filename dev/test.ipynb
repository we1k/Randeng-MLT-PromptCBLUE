{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzw/miniconda3/envs/Bert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import jieba \n",
    "from rouge_chinese import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    LlamaConfig,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    \n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "os.environ[\"WANDB_MODE\"]='disabled'\n",
    "\n",
    "\n",
    "from transformers import(\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from zero_to_fp32 import load_state_dict_from_zero_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModelForCausalLM\n",
    "\n",
    "model_name_or_path = '../model/chinese-llama-alpaca-plus-lora-7b'\n",
    "config = LlamaConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    # trust_remote_code=True\n",
    ")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    # trust_remote_code=True\n",
    ")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = \".*(1[6_9]|2[0-9]|3[0-1]).*(q_proj|k_proj|down_proj|up_proj|gate_proj)\"\n",
    "lora_rank = 8\n",
    "lora_dropout = 0.1\n",
    "lora_alpha = 32\n",
    "print(target_modules)\n",
    "print(lora_rank)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=target_modules,\n",
    "    inference_mode=False,\n",
    "    r=lora_rank, lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '../checkpoint/PromptCBLUE-alpaca-llama-7b-lora-2e-4/checkpoint-5000'\n",
    "# checkpoint = '../checkpoint/CHIP-CDEE-2e-4/checkpoint-1000'\n",
    "model = load_state_dict_from_zero_checkpoint(model, checkpoint).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('../model/global_model')\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_data_path=\"../datasets/toy_examples/\"\n",
    "train_file =  os.path.join(your_data_path, 'train.json')\n",
    "validation_file =  os.path.join(your_data_path, 'dev.json')\n",
    "test_file =  os.path.join(your_data_path, 'test.json')\n",
    "# Load dataset\n",
    "data_files = {}\n",
    "if train_file is not None:\n",
    "    data_files[\"train\"] = train_file\n",
    "    extension = train_file.split(\".\")[-1]\n",
    "if validation_file is not None:\n",
    "    data_files[\"validation\"] = validation_file\n",
    "    extension = validation_file.split(\".\")[-1]\n",
    "if test_file is not None:\n",
    "    data_files[\"test\"] = test_file\n",
    "    extension = test_file.split(\".\")[-1]\n",
    "\n",
    "lm_datasets = load_dataset(\n",
    "    extension,\n",
    "    data_files=data_files,\n",
    ")\n",
    "\n",
    "\n",
    "# Get the column names for input/target.\n",
    "prompt_column = 'input'\n",
    "response_column = 'target'\n",
    "\n",
    "column_names = lm_datasets[\"validation\"].column_names\n",
    "# Temporarily set max_target_length for training.\n",
    "max_target_length = 196\n",
    "max_input_length = 256\n",
    "prefix = ''\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    ret = [x + y for x, y in zip(examples[prompt_column], examples[response_column])]\n",
    "    return tokenizer(ret)\n",
    "\n",
    "tokenized_dataset = lm_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    # num_proc=data_args.preprocessing_num_workers,\n",
    "    num_proc=4,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Main data processing function that will make each entry its own in the dataset\n",
    "def single_texts(examples):\n",
    "    result = examples\n",
    "    result[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # block_size = data_args.max_source_length + data_args.max_target_length\n",
    "    block_size  = 520\n",
    "    \n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(single_texts, batched=True, num_proc=4)\n",
    "# lm_dataset = tokenized_dataset\n",
    "lm_dataset.set_format('torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     lm_dataset['test'],\n",
    "#     batch_size=2,\n",
    "# )\n",
    "str = \"医疗搜索：我把口香糖吃到肚子里面会不会有什么影响\\n回答内容：你好，不会有危险的，口香糖里面的主要成分是蔗糖，同时有食品胶，在体内是不会被吸收的，所以即使孩子吞服，也是不会造成不良影响的出现的。这个情况不必紧张,一般会从大便中拉出来的,以后不要给孩子吃这类食物就可以了.\\n上述搜索和回答是否相关？\\n选项: 相关，不相关\\n答：\"\n",
    "input = tokenizer(str, return_tensors='pt', padding='max_length',max_length=400)\n",
    "output = model(input['input_ids'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.argmax(output.logits[0], dim=-1)\n",
    "print(tokenizer.decode(tokens))\n",
    "# print(tokenizer.decode(tokens), '*'*50+'\\n',tokenizer.decode(input['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(\n",
    "    temperature=0.2,\n",
    "    # top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.3,\n",
    "    max_new_tokens=400\n",
    ")\n",
    "output = model.generate(input['input_ids'], **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    # '../model/ChatMed_llama/'\n",
    "    '../model/chinese-llama-alpaca-plus-lora-7b/',\n",
    "    config=config,\n",
    ").half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzw/miniconda3/envs/Bert/lib/python3.8/site-packages/transformers/generation/utils.py:1253: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device='cpu'\n",
    "\n",
    "str = f\"\"\"\n",
    "### 指令:\n",
    "根据下文判断上述搜索和回答是否相关？\n",
    "### 输入:\n",
    "指令：根据下文判断上述搜索和回答是否相关？回答: 相关,不相关\\n医疗搜索：我把口香糖吃到肚子里面会不会有什么影响\\n回答内容：你好，不会有危险的，口香糖里面的主要成分是蔗糖，同时有食品胶，在体内是不会被吸收的，所以即使孩子吞服，也是不会造成不良影响的出现的。这个情况不必紧张,一般会从大便中拉出来的,以后不要给孩子吃这类食物就可以了.\\n\\n\\n答：\n",
    "### 输出:\"\"\"\n",
    "\n",
    "\n",
    "generation_config = dict(\n",
    "    temperature=0.2,\n",
    "    # top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.3,\n",
    "    max_new_tokens=400\n",
    ")\n",
    "\n",
    "inputs = tokenizer(str, return_tensors='pt', padding='max_length',max_length=400, )\n",
    "output = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"].to(device),\n",
    "    attention_mask=inputs['attention_mask'].to(device),\n",
    "    **generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 指令:\n",
      "根据下文判断上述搜索和回答是否相关？\n",
      "### 输入:\n",
      "指令：根据下文判断上述搜索和回答是否相关？回答: 相关,不相关\n",
      "医疗搜索：我把口香糖吃到肚子里面会不会有什么影响\n",
      "回答内容：你好，不会有危险的，口香糖里面的主要成分是蔗糖，同时有食品胶，在体内是不会被吸收的，所以即使孩子吞服，也是不会造成不良影响的出现的。这个情况不必紧张,一般会从大便中拉出来的,以后不要给孩子吃这类食物就可以了.\n",
      "\n",
      "\n",
      "答：\n",
      "### 输出:  相关的 。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。  。茬  。茬  。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬    。  。茬\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)\n",
    "# response = output.split(\"答：\\n\")[1].strip()\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
